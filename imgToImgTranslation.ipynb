{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeh-nix/manga-colourization/blob/master/imgToImgTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQTvUFBPufAn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# !pip install IPython\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# from keras import tf.keras.layers\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K13iv54axpEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2672f190-9af2-480d-a603-69da2d59469e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RRG_DuGufAs"
      },
      "source": [
        "### Path to Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ_rHtQPufAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a378d5-4f40-4bac-ac3b-8cdcc95be9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dataset/test\n",
            "/content/drive/MyDrive/dataset/colored full\n",
            "/content/drive/MyDrive/dataset/grayscaled full\n",
            "/content/drive/MyDrive/dataset/grayscaled\n",
            "/content/drive/MyDrive/dataset/colored\n"
          ]
        }
      ],
      "source": [
        "# dataset_path = pathlib.Path(\n",
        "#     \"./dataset\"\n",
        "# )\n",
        "dataset_path = pathlib.Path(\n",
        "    \"/content/drive/MyDrive/dataset\"\n",
        ")\n",
        "\n",
        "for folder in dataset_path.iterdir():\n",
        "    print(folder)\n",
        "\n",
        "# Path to colored and grayscaled image folders\n",
        "colored_path = dataset_path / \"colored\"\n",
        "grayscaled_path = dataset_path / \"grayscaled\"\n",
        "dataset_test = dataset_path / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXqK3acvufAv"
      },
      "source": [
        "### Fetch a sample image from the colored folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbA4YGNKufAv"
      },
      "outputs": [],
      "source": [
        "# sample_colored_image = list(colored_path.glob(\"*\"))[0]\n",
        "# print(sample_colored_image)\n",
        "# image = tf.io.read_file(str(sample_colored_image))\n",
        "# image = tf.image.decode_image(image)\n",
        "# # plt.figure()\n",
        "# # plt.imshow(image)\n",
        "# # plt.title(\"Sample Colored Image\")\n",
        "# # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A57w1-5ufAw"
      },
      "source": [
        "### Fetch a sample image from the grayscaled folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdf8sm_-ufAw"
      },
      "outputs": [],
      "source": [
        "# sample_grayscaled_image = list(grayscaled_path.glob(\"*\"))[0]  # Fetch the first image\n",
        "# print(sample_grayscaled_image)\n",
        "# image = tf.io.read_file(str(sample_grayscaled_image))\n",
        "# image = tf.image.decode_image(image)\n",
        "# print(type(image))\n",
        "# plt.figure()\n",
        "# plt.imshow(image, cmap=\"gray\")\n",
        "# plt.title(\"Sample Grayscaled Image\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc4zxPzuufAx"
      },
      "source": [
        "### Loading the files and normalizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0LT0YuIufAy"
      },
      "outputs": [],
      "source": [
        "def load(input_image_file, real_image_file):\n",
        "    input_image = tf.io.read_file(input_image_file)\n",
        "    input_image = tf.image.decode_jpeg(input_image, channels=1)\n",
        "    input_image = tf.cast(input_image, tf.float32)\n",
        "    input_image = input_image / 255.0\n",
        "\n",
        "    real_image = tf.io.read_file(real_image_file)\n",
        "    real_image = tf.image.decode_jpeg(real_image, channels=3)\n",
        "    real_image = tf.cast(real_image, tf.float32)\n",
        "    real_image = real_image / 255.0\n",
        "    # print(\"input: \", input_image_file, \" real: \", real_image_file)\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "\n",
        "\n",
        "# Testing Load input and real images\n",
        "\n",
        "# input_image, real_image = load(\n",
        "#     f\"{grayscaled_path}/1_31.jpg\", f\"{colored_path}/1_31.jpg\"\n",
        "\n",
        "# )\n",
        "\n",
        "\n",
        "# print(type(input_image))\n",
        "# print(type(real_image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIiaGitYufAy"
      },
      "source": [
        "### Cast images for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6txgXYz1ufAz"
      },
      "outputs": [],
      "source": [
        "# # Assuming input_image and real_image are TensorFlow eager tensors\n",
        "# # Convert them to NumPy arrays\n",
        "# # input_image_np = input_image.numpy()\n",
        "# # real_image_np = real_image.numpy()\n",
        "\n",
        "# # Plot the images\n",
        "# plt.figure()\n",
        "# # plt.imshow(input_image_np[:, :, 0], cmap=\"gray\")\n",
        "# plt.imshow(input_image, cmap=\"gray\")\n",
        "# plt.title(\"Input Image\")\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure()\n",
        "# # plt.imshow(real_image_np[:, :, :])\n",
        "# plt.imshow(real_image)\n",
        "# plt.title(\"Real Image\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-p50WQnufAz"
      },
      "source": [
        "### Skipping the \"As described in the [pix2pix](https://arxiv.org/abs/1611.07004) paper, you need to apply random jittering and mirroring to preprocess the training set.\" part of preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e03CT0t0ufAz"
      },
      "source": [
        "### Fetch the file paths for grayscale and colored images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW9bxmRqufA0"
      },
      "outputs": [],
      "source": [
        "grayscale_images = tf.data.Dataset.list_files(f\"{grayscaled_path}/*.jpg\", shuffle=False)\n",
        "colored_images = tf.data.Dataset.list_files(f\"{colored_path}/*.jpg\", shuffle=False)\n",
        "\n",
        "paired_images = tf.data.Dataset.zip((grayscale_images, colored_images))\n",
        "paired_images = paired_images.map(load)\n",
        "\n",
        "# BATCH_SIZE = 32\n",
        "BATCH_SIZE = 16\n",
        "# Adjust buffer size as needed for shuffling (at least > than your len(dataset))\n",
        "BUFFER_SIZE = 550\n",
        "paired_images = paired_images.shuffle(BUFFER_SIZE)\n",
        "paired_images = paired_images.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xYRcFR1ufA0"
      },
      "outputs": [],
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters,\n",
        "            size,\n",
        "            strides=2,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=initializer,\n",
        "            use_bias=False,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# down_model = downsample(3, 4)\n",
        "# down_result = down_model(tf.expand_dims(input_image, 0))\n",
        "# print(down_result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXZGSuwOufA3"
      },
      "outputs": [],
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "        tf.keras.layers.Conv2DTranspose(\n",
        "            filters,\n",
        "            size,\n",
        "            strides=2,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=initializer,\n",
        "            use_bias=False,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "    result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# up_model = upsample(3, 4)\n",
        "# up_result = up_model(down_result)\n",
        "# print(up_result.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H49e20ZrufA4"
      },
      "source": [
        "### Build the Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46oWQ4b3ufA5"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "    inputs = tf.keras.layers.Input(\n",
        "        shape=(256, 256, 1)\n",
        "    )  # Input shape for grayscale images\n",
        "\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_batchnorm=True),\n",
        "        downsample(128, 4),\n",
        "        downsample(256, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4),\n",
        "        upsample(256, 4),\n",
        "        upsample(128, 4),\n",
        "        upsample(64, 4),\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
        "    last = tf.keras.layers.Conv2DTranspose(\n",
        "        3,  # Output has 3 channels (RGB)\n",
        "        4,\n",
        "        strides=2,\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=initializer,\n",
        "        activation=\"tanh\",\n",
        "    )\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "\n",
        "# Create the generator\n",
        "# generator = build_generator()\n",
        "# gen_output = generator(input_image[tf.newaxis, ..., tf.newaxis], training=False)\n",
        "\n",
        "# Assuming gen_output is in the range [-1, 1]\n",
        "# gen_output = (gen_output + 1) / 2.0\n",
        "\n",
        "# plt.imshow(gen_output[0, ...])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV-cg9wVufA6"
      },
      "outputs": [],
      "source": [
        "LAMBDA = 100\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    # Mean absolute error\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "    return total_gen_loss, gan_loss, l1_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZPyPtF3ufA6"
      },
      "source": [
        "[The Training Procedure for the generator(image)](https://raw.githubusercontent.com/tensorflow/docs/master/site/en/tutorials/generative/images/gen.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gwybzXNufA6"
      },
      "source": [
        "### Build the discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edD9tbQFufA6"
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
        "\n",
        "    inp = tf.keras.layers.Input(\n",
        "        shape=[256, 256, 1], name=\"input_image\"\n",
        "    )  # Grayscale image\n",
        "    tar = tf.keras.layers.Input(\n",
        "        shape=[256, 256, 3], name=\"target_image\"\n",
        "    )  # Colored image\n",
        "\n",
        "    x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
        "\n",
        "    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
        "    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
        "    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
        "\n",
        "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
        "    conv = tf.keras.layers.Conv2D(\n",
        "        512, 4, strides=1, kernel_initializer=initializer, use_bias=False\n",
        "    )(\n",
        "        zero_pad1\n",
        "    )  # (batch_size, 31, 31, 512)\n",
        "\n",
        "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
        "\n",
        "    last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(\n",
        "        zero_pad2\n",
        "    )  # (batch_size, 30, 30, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y_cwP4QufA7"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    # Adversarial loss for real images\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(\n",
        "        tf.zeros_like(disc_generated_output), disc_generated_output\n",
        "    )\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn6AVBj_ufA7"
      },
      "source": [
        "### Test the Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDW5IeTUufA7"
      },
      "outputs": [],
      "source": [
        "# discriminator = Discriminator()\n",
        "# disc_out = discriminator([input_image, gen_output], training=False)\n",
        "# plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap=\"RdBu_r\")\n",
        "# plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSeBLOYHufA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c78c093-5004-4ff2-fcc9-b4f1bc70b311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 87.0942153930664, GAN Loss: 1.0221229791641235, L1 Loss: 0.8607209324836731\n",
            "Discriminator Loss: 1.6991660594940186\n"
          ]
        }
      ],
      "source": [
        "# Initialize the generator and discriminator models\n",
        "generator = build_generator()\n",
        "discriminator = Discriminator()\n",
        "# Sample input and target\n",
        "input_image, real_image = load(\n",
        "    f\"{grayscaled_path}/1_59.jpg\", f\"{colored_path}/1_59.jpg\"\n",
        "    # f\"{grayscaled_path}/1_31.jpg\", f\"{colored_path}/1_31.jpg\"\n",
        ")\n",
        "input_image = tf.expand_dims(input_image, axis=0)\n",
        "real_image = tf.expand_dims(real_image, axis=0)\n",
        "\n",
        "# Generate output with the generator\n",
        "gen_output = generator(input_image, training=True)\n",
        "\n",
        "# Get discriminator outputs\n",
        "disc_real_output = discriminator([input_image, real_image], training=True)\n",
        "disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "# Calculate losses\n",
        "gen_loss, gan_loss, l1_loss = generator_loss(\n",
        "    disc_generated_output, gen_output, real_image\n",
        ")\n",
        "disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "print(\n",
        "    f\"Generator Loss: {gen_loss.numpy()}, GAN Loss: {gan_loss.numpy()}, L1 Loss: {l1_loss.numpy()}\"\n",
        ")\n",
        "print(f\"Discriminator Loss: {disc_loss.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4luhNXVufA8"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npcf2NfRufA8"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    generator_optimizer=generator_optimizer,\n",
        "    discriminator_optimizer=discriminator_optimizer,\n",
        "    generator=generator,\n",
        "    discriminator=discriminator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpHLchGcufA8"
      },
      "outputs": [],
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "    prediction = model(test_input, training=True)\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    display_list = [test_input[0], tar[0], prediction[0]]\n",
        "    title = [\"Input Image\", \"Ground Truth\", \"Predicted Image\"]\n",
        "\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(title[i])\n",
        "        # Getting the pixel values in the [0, 1] range to plot.\n",
        "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyNCOd7TufA9"
      },
      "outputs": [],
      "source": [
        "# for example_input, example_target in test_dataset.take(1):\n",
        "generate_images(generator, test_input=input_image, tar=real_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a5DVsjDufA9"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbR9dAfgufA9"
      },
      "outputs": [],
      "source": [
        "log_dir = \"logs/\"\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "    log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz49iVwVufA9"
      },
      "outputs": [],
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "# @tf.function\n",
        "def train_step(input_image, target, step):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        gen_output = generator(input_image, training=True)\n",
        "\n",
        "        disc_real_output = discriminator([input_image, target], training=True)\n",
        "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(\n",
        "            disc_generated_output, gen_output, target\n",
        "        )\n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "    generator_gradients = gen_tape.gradient(\n",
        "        gen_total_loss, generator.trainable_variables\n",
        "    )\n",
        "    discriminator_gradients = disc_tape.gradient(\n",
        "        disc_loss, discriminator.trainable_variables\n",
        "    )\n",
        "\n",
        "    generator_optimizer.apply_gradients(\n",
        "        zip(generator_gradients, generator.trainable_variables)\n",
        "    )\n",
        "    discriminator_optimizer.apply_gradients(\n",
        "        zip(discriminator_gradients, discriminator.trainable_variables)\n",
        "    )\n",
        "    # print(\"gen_total_loss\", gen_total_loss, step=step // 1000)\n",
        "    # print(\"gen_gan_loss\", gen_gan_loss, step=step // 1000)\n",
        "    # print(\"gen_l1_loss\", gen_l1_loss, step=step // 1000)\n",
        "    # print(\"disc_loss\", disc_loss, step=step // 1000)\n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar(\"gen_total_loss\", gen_total_loss, step=step // 1000)\n",
        "        tf.summary.scalar(\"gen_gan_loss\", gen_gan_loss, step=step // 1000)\n",
        "        tf.summary.scalar(\"gen_l1_loss\", gen_l1_loss, step=step // 1000)\n",
        "        tf.summary.scalar(\"disc_loss\", disc_loss, step=step // 1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d5LPQyaufA-"
      },
      "outputs": [],
      "source": [
        "def fit(train_ds, test_ds, steps):\n",
        "    example_input, example_target = next(iter(test_ds.take(1)))\n",
        "    start = time.time()\n",
        "\n",
        "    for step, (input_image, target) in enumerate(train_ds.take(steps)):\n",
        "        if (step + 1) % 1000 == 0:\n",
        "            display.clear_output(wait=True)\n",
        "\n",
        "            if step != 0:\n",
        "                print(f\"Time taken for 1000 steps: {time.time()-start:.2f} sec\\n\")\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            generate_images(generator, example_input, example_target)\n",
        "            print(f\"Step: {step//1000}k\")\n",
        "\n",
        "        train_step(input_image, target, step)\n",
        "\n",
        "        if (step + 1) % 10 == 0:\n",
        "            print(\".\", end=\"\", flush=True)\n",
        "\n",
        "        if (step + 1) % 5 == 0:\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def fit(train_ds, test_ds, steps):\n",
        "#   example_input, example_target = next(iter(test_ds.take(1)))\n",
        "#   start = time.time()\n",
        "\n",
        "#   for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
        "#     if (step) % 1000 == 0:\n",
        "#       display.clear_output(wait=True)\n",
        "\n",
        "#       if step != 0:\n",
        "#         print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "#       start = time.time()\n",
        "\n",
        "#       generate_images(generator, example_input, example_target)\n",
        "#       print(f\"Step: {step//1000}k\")\n",
        "\n",
        "#     train_step(input_image, target, step)\n",
        "\n",
        "#     # Training step\n",
        "#     if (step+1) % 10 == 0:\n",
        "#       print('.', end='', flush=True)\n",
        "\n",
        "\n",
        "#     # Save (checkpoint) the model every 5k steps\n",
        "#     if (step + 1) % 5000 == 0:\n",
        "#       checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "zlMIHotFRxG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# DELETE THE CHECKPOINTS TO RELEASE MEMORHY\n",
        "# def force_delete_directory_contents(directory):\n",
        "#     for filename in os.listdir(directory):\n",
        "#         file_path = os.path.join(directory, filename)\n",
        "#         try:\n",
        "#             if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "#                 os.unlink(file_path)\n",
        "#             elif os.path.isdir(file_path):\n",
        "#                 shutil.rmtree(file_path)\n",
        "#         except Exception as e:\n",
        "#             print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
        "\n",
        "# # Specify the directory you want to clear\n",
        "# checkpoint_dir = \"./training_checkpoints\"\n",
        "\n",
        "# # Call the function to delete the contents of the directory\n",
        "# force_delete_directory_contents(checkpoint_dir)"
      ],
      "metadata": {
        "id": "uzUD0qdlDi9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWKmnvWAufA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8f217f-6e11-4f8c-c9f6-8441468986ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access './training_checkpoints': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls {checkpoint_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT3-yBP0ufA_"
      },
      "outputs": [],
      "source": [
        "# Restoring the latest checkpoint in checkpoint_dir\n",
        "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcfmPIXHCbdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec9893af-aa02-406b-8e0b-c688a48b2e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
            "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n"
          ]
        }
      ],
      "source": [
        "# Run the trained model on a few examples from the test set\n",
        "# for inp, tar in test_dataset.take(5):\n",
        "# generate_images(generator, inp, tar)\n",
        "# generate_images(generator, test_input=input_image, tar=real_image)\n",
        "# for inp, tar in dataset_test.take(5):\n",
        "#     generate_images(generator, inp, tar)\n",
        "grayscale_images = tf.data.Dataset.list_files(f\"{grayscaled_path}/*.jpg\", shuffle=False)\n",
        "colored_images = tf.data.Dataset.list_files(f\"{colored_path}/*.jpg\", shuffle=False)\n",
        "\n",
        "paired_images = tf.data.Dataset.zip((grayscale_images, colored_images))\n",
        "paired_images = paired_images.map(load)\n",
        "\n",
        "# BATCH_SIZE = 32\n",
        "BATCH_SIZE = 16\n",
        "# Adjust buffer size as needed for shuffling (atleast > than your len(dataset)) in my case I have 5709 colored and grayscale images\n",
        "BUFFER_SIZE = 550\n",
        "paired_images = paired_images.shuffle(BUFFER_SIZE)\n",
        "paired_images = paired_images.batch(BATCH_SIZE)\n",
        "print(grayscale_images)\n",
        "print(colored_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxElhImkufA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98026ca4-f422-4169-d2ca-e4b87ffd5c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function train_step at 0x7fee50638ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 16 calls to <function train_step at 0x7fee50638ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fee3823ba00>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Restoring the latest checkpoint in checkpoint_dir\n",
        "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "# fit(train_dataset, test_dataset, steps=40000)\n",
        "fit(paired_images, paired_images, steps=50)\n",
        "\n",
        "# Restore the latest checkpoint\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJfwe78_ufBG"
      },
      "outputs": [],
      "source": [
        "def generate_and_display_images(model, test_input, tar):\n",
        "    # Generate images using the generator\n",
        "    gen_output = model(test_input, training=False)\n",
        "\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    display_list = [test_input[0], tar[0], gen_output[0]]\n",
        "    title = [\"Input Image\", \"Target Image\", \"Generated Image\"]\n",
        "    # print(display_list)\n",
        "\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(title[i])\n",
        "\n",
        "        # Ensure pixel values are in the range [0, 1] (Normalizing)\n",
        "        plt.imshow(display_list[i] * 0.5 + 0.5, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "for test_input, target in paired_images.take(5):\n",
        "    generate_and_display_images(generator, test_input, target)\n",
        "\n",
        "# Restore the latest checkpoint\n",
        "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgNBe75zbTS1"
      },
      "outputs": [],
      "source": [
        "# def generate_and_save_images(model, test_input, tar, epoch):\n",
        "#     gen_output = model(test_input, training=False)\n",
        "\n",
        "#     plt.figure(figsize=(15, 15))\n",
        "\n",
        "#     display_list = [test_input[0], tar[0], gen_output[0]]\n",
        "#     title = [\"Input Image\", \"Target Image\", \"Generated Image\"]\n",
        "\n",
        "#     for i in range(3):\n",
        "#         plt.subplot(1, 3, i + 1)\n",
        "#         plt.title(title[i])\n",
        "\n",
        "#         plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "#         plt.axis(\"off\")\n",
        "\n",
        "#     plt.savefig(f\"generated_image_at_epoch_{epoch}.png\")\n",
        "#     plt.show()\n",
        "\n",
        "# # # Example usage inside your training loop\n",
        "# # for epoch in range(epochs):\n",
        "# #     for train_input, train_target in train_dataset:\n",
        "# #         train_step(train_input, train_target)\n",
        "\n",
        "# #     # Generate and save test images at the end of each epoch\n",
        "# #     for test_input, test_target in test_dataset:\n",
        "# #         generate_and_save_images(generator, test_input, test_target, epoch)\n",
        "\n",
        "# for test_input, target in paired_images.take(5):\n",
        "#     generate_and_display_images(generator, test_input, target)\n",
        "\n",
        "# # Restore the latest checkpoint\n",
        "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYu3-wC7STF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}